{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치(PyTorch)의 nn.Embedding()\n",
    "임베딩 벡터를 사용하는 방법이 크게 두 가지\n",
    "- 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법\n",
    "- 미리 사전에 훈련된 임베딩 벡터(pre-trained word embedding)들을 가져와 사용하는 방법\n",
    "\n",
    "## 1. 학습하여 사용\n",
    "\n",
    "### 임베딩 층은 룩업 테이블\n",
    "- 임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야\n",
    "- 어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터\n",
    "\n",
    "### nn.Embedding()을 사용하지 않고 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'you need to know how to code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 2, 'know': 3, 'need': 4, 'to': 5, 'how': 6, 'code': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "source": [
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합의 크기만큼의 행을 가지는 테이블 생성.\n",
    "embedding_table = torch.FloatTensor([\n",
    "                               [ 0.0,  0.0,  0.0],\n",
    "                               [ 0.0,  0.0,  0.0],\n",
    "                               [ 0.2,  0.9,  0.3],\n",
    "                               [ 0.1,  0.5,  0.7],\n",
    "                               [ 0.2,  0.1,  0.8],\n",
    "                               [ 0.4,  0.1,  0.1],\n",
    "                               [ 0.1,  0.8,  0.9],\n",
    "                               [ 0.6,  0.1,  0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.9000, 0.3000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.4000, 0.1000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플 문장\n",
    "sample = 'you need to run'.split()\n",
    "idxes=[]\n",
    "# 각 단어를 정수로 변환\n",
    "for word in sample:\n",
    "  try:\n",
    "    idxes.append(vocab[word])\n",
    "  except KeyError: # 단어 집합에 없는 단어일 경우 <unk>로 대체된다.\n",
    "    idxes.append(vocab['<unk>'])\n",
    "idxes = torch.LongTensor(idxes)\n",
    "\n",
    "# 룩업 테이블\n",
    "lookup_result = embedding_table[idxes, :] # 각 정수를 인덱스로 임베딩 테이블에서 값을 가져온다.\n",
    "print(lookup_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "embedding_layer = nn.Embedding(num_embeddings = len(vocab), \n",
    "                               embedding_dim = 3,\n",
    "                               padding_idx = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding은 크게 두 가지 인자를 받음 :  num_embeddings과 embedding_dim입니다.\n",
    "\n",
    "- num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기\n",
    "- embedding_dim : 임베딩 할 벡터의 차원. 사용자가 정해주는 하이퍼파라미터\n",
    "- padding_idx : 선택적으로 사용하는 인자. 패딩을 위한 토큰의 인덱스\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.8625,  1.7961, -0.1986],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.8048,  0.7859,  0.2050],\n",
      "        [ 0.7183, -1.2849,  0.1338],\n",
      "        [ 0.3679, -0.4931, -0.5602],\n",
      "        [-0.5691, -0.3030, -1.0060],\n",
      "        [ 1.5824, -0.1845, -0.0263],\n",
      "        [ 0.7893,  1.3310, -1.7532]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.사전 훈련된 워드 임베딩(Pretrained Word Embedding)\n",
    "\n",
    "### 2.1 IMDB 리뷰 데이터를 훈련 데이터로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 개의 Field 객체를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HunbeomBak\\.conda\\envs\\nlp\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext.datasets은 IMDB, TREC(질문 분류), 언어 모델링(WikiText-2) 등 다른 여러 데이터셋을 제공   \n",
    "torchtext.datasets을 사용해 IMDB 데이터셋을 다운로드하고, 이 데이터셋을 학습 데이터셋과 테스트 데이터셋으로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HunbeomBak\\.conda\\envs\\nlp\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : 25000\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 크기 : {}' .format(len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"teachers\".', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"teachers\".', 'the', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'i', 'immediately', 'recalled', '.........', 'at', '..........', 'high.', 'a', 'classic', 'line:', 'inspector:', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'student:', 'welcome', 'to', 'bromwell', 'high.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched.', 'what', 'a', 'pity', 'that', 'it', \"isn't!\"], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(trainset[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 토치텍스트를 사용한 사전 훈련된 워드 임베딩\n",
    "\n",
    "\n",
    "#### 2.2.1 사전 훈련된 Word2Vec 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 'eng_w2v' 모델을 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format('eng_w2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46345907 -1.1891901   1.245648   -0.37404516  1.652432   -0.5652389\n",
      " -0.47939372 -0.12096556 -0.11520188 -0.06890516  2.2709115   1.1223398\n",
      "  1.5259689   1.1311605   0.5108806  -0.23671353  1.0038161  -0.23612505\n",
      " -0.28746054 -0.95729315 -0.74801165 -0.7392374  -0.7818662   0.49504504\n",
      "  0.17349571 -2.360132    0.7226233  -0.15720506 -0.99177873  0.3037044\n",
      "  1.8142542   0.5037929  -0.9390487   0.98808104 -0.05130086 -0.59606653\n",
      " -1.1025496  -0.6204695  -0.5616367   0.46749592 -0.15358241  1.6946322\n",
      "  1.3283756   0.24887817  0.3837215   1.9791477  -0.93284166  0.75411063\n",
      " -0.6865323   0.42933154  1.1695147   2.311801    1.6639466  -0.6908743\n",
      "  2.5462062  -1.4810184  -0.16076116 -0.47009483  1.0839101  -0.12877102\n",
      "  0.49614483 -1.7158998   1.303417   -0.24069652  0.4524425  -2.2316554\n",
      " -1.0992208  -2.5734618  -0.90478575  0.47986376  0.9350458   0.8525662\n",
      "  0.74907756  1.6617146  -0.41617632  1.311047    0.06830977 -0.7097945\n",
      " -1.2986578   0.01954382 -0.7358456  -0.82133025 -1.7464945   1.5272204\n",
      " -0.10501689 -2.0723665  -0.1302002  -1.4819938   0.83955795  0.0404703\n",
      " -1.1909548  -0.7786654   0.05790685 -0.38648108  1.0886759   0.848424\n",
      " -1.430856    1.7356222   0.293956    0.6262219 ]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model['this']) # 영어 단어 'this'의 임베딩 벡터값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'self-indulgent' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a0b978553c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'self-indulgent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 영어 단어 'self-indulgent'의 임베딩 벡터값 출력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'self-indulgent' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(word2vec_model['self-indulgent']) # 영어 단어 'self-indulgent'의 임베딩 벡터값 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Word2Vec 학습시에 존재하지 않았던 단어이므로 'self-indulgent'의 임베딩 벡터값을 갖고 있지 않다는 에러"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 사전 훈련된 Word2Vec을 초기 임베딩으로 사용\n",
    "pre-trained된 임베딩 벡터들을 IMDB 리뷰 데이터의 단어들에 맵핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/21613 [00:00<?, ?it/s]Skipping token b'21613' with 1-dimensional vector [b'100']; likely a header\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 21613/21613 [00:00<00:00, 31493.36it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = Vectors(name=\"eng_w2v\") # 사전 훈련된 Word2Vec 모델을 vectors에 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field 객체의 build_vocab을 통해 훈련 데이터의 단어 집합(vocabulary)를 만드는 것과 동시에 임베딩 벡터값들을 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset, vectors=vectors, max_size=10000, min_freq=10) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_size와 min_freq는 몇 개의 단어들만을 가지고 단어 집합을 생성할 것인지를 정합니다. \n",
    "- max_size는 단어 집합의 크기를 제한하고, min_freq=10은 등장 빈도수가 10번 이상인 단어만 허용하는 것한다는 의미입니다. \n",
    "- vectors=vectors는 만들어진 단어 집합의 각 단어의 임베딩 벡터값으로 env_w2v에 저장되어져 있던 임베딩 벡터값들로 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.stoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수와 차원 : torch.Size([10002, 100]) \n"
     ]
    }
   ],
   "source": [
    "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,002개의 단어 : 훈련 데이터 단어 10000개 + < unk >와 < pad >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[0]) # <unk>의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[1]) # <pad>의 임베딩 벡터값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4635, -1.1892,  1.2456, -0.3740,  1.6524, -0.5652, -0.4794, -0.1210,\n",
      "        -0.1152, -0.0689,  2.2709,  1.1223,  1.5260,  1.1312,  0.5109, -0.2367,\n",
      "         1.0038, -0.2361, -0.2875, -0.9573, -0.7480, -0.7392, -0.7819,  0.4950,\n",
      "         0.1735, -2.3601,  0.7226, -0.1572, -0.9918,  0.3037,  1.8143,  0.5038,\n",
      "        -0.9390,  0.9881, -0.0513, -0.5961, -1.1025, -0.6205, -0.5616,  0.4675,\n",
      "        -0.1536,  1.6946,  1.3284,  0.2489,  0.3837,  1.9791, -0.9328,  0.7541,\n",
      "        -0.6865,  0.4293,  1.1695,  2.3118,  1.6639, -0.6909,  2.5462, -1.4810,\n",
      "        -0.1608, -0.4701,  1.0839, -0.1288,  0.4961, -1.7159,  1.3034, -0.2407,\n",
      "         0.4524, -2.2317, -1.0992, -2.5735, -0.9048,  0.4799,  0.9350,  0.8526,\n",
      "         0.7491,  1.6617, -0.4162,  1.3110,  0.0683, -0.7098, -1.2987,  0.0195,\n",
      "        -0.7358, -0.8213, -1.7465,  1.5272, -0.1050, -2.0724, -0.1302, -1.4820,\n",
      "         0.8396,  0.0405, -1.1910, -0.7787,  0.0579, -0.3865,  1.0887,  0.8484,\n",
      "        -1.4309,  1.7356,  0.2940,  0.6262])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[10]) # this의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[10000]) # 단어 'self-indulgent'의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 임베딩 벡터들을 nn.Embedding()의 초기화 입력으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4635, -1.1892,  1.2456, -0.3740,  1.6524, -0.5652, -0.4794, -0.1210,\n",
      "         -0.1152, -0.0689,  2.2709,  1.1223,  1.5260,  1.1312,  0.5109, -0.2367,\n",
      "          1.0038, -0.2361, -0.2875, -0.9573, -0.7480, -0.7392, -0.7819,  0.4950,\n",
      "          0.1735, -2.3601,  0.7226, -0.1572, -0.9918,  0.3037,  1.8143,  0.5038,\n",
      "         -0.9390,  0.9881, -0.0513, -0.5961, -1.1025, -0.6205, -0.5616,  0.4675,\n",
      "         -0.1536,  1.6946,  1.3284,  0.2489,  0.3837,  1.9791, -0.9328,  0.7541,\n",
      "         -0.6865,  0.4293,  1.1695,  2.3118,  1.6639, -0.6909,  2.5462, -1.4810,\n",
      "         -0.1608, -0.4701,  1.0839, -0.1288,  0.4961, -1.7159,  1.3034, -0.2407,\n",
      "          0.4524, -2.2317, -1.0992, -2.5735, -0.9048,  0.4799,  0.9350,  0.8526,\n",
      "          0.7491,  1.6617, -0.4162,  1.3110,  0.0683, -0.7098, -1.2987,  0.0195,\n",
      "         -0.7358, -0.8213, -1.7465,  1.5272, -0.1050, -2.0724, -0.1302, -1.4820,\n",
      "          0.8396,  0.0405, -1.1910, -0.7787,  0.0579, -0.3865,  1.0887,  0.8484,\n",
      "         -1.4309,  1.7356,  0.2940,  0.6262]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.LongTensor([10]))) # 단어 this의 임베딩 벡터값\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 토치텍스트에서 제공하는 사전 훈련된 워드 임베딩\n",
    "\n",
    "- fasttext.en.300d\n",
    "- fasttext.simple.300d\n",
    "- glove.42B.300d\n",
    "- glove.840B.300d\n",
    "- glove.twitter.27B.25d\n",
    "- glove.twitter.27B.50d\n",
    "- glove.twitter.27B.100d\n",
    "- glove.twitter.27B.200d\n",
    "- glove.6B.50d\n",
    "- glove.6B.100d\n",
    "- glove.6B.200d\n",
    "- glove.6B.300d <= 이거 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field 객체의 build_vocab을 통해 토치텍스트가 제공하는 사전 훈련된 임베딩 벡터를 다운받을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [06:36, 2.17MB/s]                                                                    \n",
      "100%|██████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:29<00:00, 13528.60it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(trainset, vectors=GloVe(name='6B', dim=300), max_size=10000, min_freq=10)\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.stoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수와 차원 : torch.Size([10002, 300]) \n"
     ]
    }
   ],
   "source": [
    "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[0]) # <unk>의 임베딩 벡터값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[1]) # <pad>의 임베딩 벡터값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0437e-01,  1.6431e-01,  4.1794e-02, -1.3708e-01, -2.9779e-01,\n",
      "         3.3440e-01, -6.9955e-02, -6.8036e-02,  1.0604e-01, -2.0337e+00,\n",
      "         1.7977e-01, -7.7403e-02, -1.9518e-01,  1.8324e-01,  3.0017e-02,\n",
      "        -5.4762e-02, -4.5725e-01, -2.4509e-02,  5.7387e-02, -3.4878e-01,\n",
      "         3.9696e-02,  4.4826e-01, -5.8462e-02,  4.1181e-01, -3.5411e-02,\n",
      "        -1.4722e-01,  1.0740e-01, -2.5896e-01, -1.1658e-01,  1.9822e-01,\n",
      "         3.2850e-01,  2.4177e-01, -5.7177e-01, -5.6442e-02, -9.6437e-01,\n",
      "         3.4482e-01,  5.4639e-02,  2.3828e-01, -1.9139e-01,  3.0899e-01,\n",
      "         2.8044e-01, -3.3814e-02, -2.5436e-01,  1.5373e-02,  1.6341e-01,\n",
      "         2.6352e-01,  1.5812e-01,  3.2044e-01, -2.3082e-01,  2.6050e-01,\n",
      "         2.0606e-01, -8.9769e-02, -1.0055e-01,  7.0378e-02, -2.7452e-02,\n",
      "         2.7959e-01, -9.5862e-02,  2.0574e-01,  2.9522e-01, -1.2285e-02,\n",
      "         1.1164e-01, -5.1373e-02,  4.6106e-01,  2.3014e-02, -3.7141e-01,\n",
      "        -2.4166e-01,  3.3773e-02,  3.6827e-02,  1.6918e-01, -1.0802e-01,\n",
      "        -1.0691e-01,  1.0219e-01,  1.0065e-01,  5.5907e-02, -2.7402e-01,\n",
      "        -1.3689e-01,  4.2095e-01, -2.4060e-02, -3.2099e-01,  3.2065e-01,\n",
      "        -1.6776e-01,  1.0170e-01,  7.4999e-02, -1.0486e-01,  3.7114e-01,\n",
      "         3.2972e-01, -3.3043e-01,  3.8343e-01,  2.4555e-01,  2.0386e-01,\n",
      "        -4.1919e-01,  1.1570e-01, -7.8632e-02, -4.3171e-01, -2.3550e-02,\n",
      "        -1.1618e-01, -2.5605e-01,  3.4693e-01,  2.0398e-01, -1.7857e-01,\n",
      "         1.7301e-01,  4.6408e-02, -1.0486e-01,  9.8706e-02, -3.2077e-02,\n",
      "        -3.0462e-01,  1.2826e-01,  6.7985e-02, -2.5993e-01,  3.8041e-01,\n",
      "         4.5252e-02, -9.1834e-02, -4.5206e-01, -1.2498e-01,  1.7515e-01,\n",
      "        -1.3551e-01, -2.0556e-03, -9.3906e-02, -2.8006e-02, -4.6975e-01,\n",
      "         8.4430e-03, -2.4092e-01,  1.6000e-01,  2.2063e-01,  3.6277e-01,\n",
      "        -6.7643e-02,  2.8755e-01,  1.2643e-01,  1.2202e-01,  1.0548e-01,\n",
      "         4.0249e-01,  2.9781e-01,  4.9507e-01, -1.1096e-01, -2.4472e-01,\n",
      "         1.8720e-01,  1.1156e-01,  1.5680e-02,  7.6296e-03,  1.4304e-01,\n",
      "        -2.9299e-01,  1.7912e-01,  1.1604e-02, -5.6776e-02, -5.0224e-01,\n",
      "        -4.7262e-01,  1.5790e-01,  3.1573e-01, -7.7839e-02,  3.5172e-01,\n",
      "         1.6097e-01, -2.2595e-01,  2.4629e-01, -3.8200e-02,  6.4918e-01,\n",
      "         5.9545e-02, -5.0641e-02,  1.9511e-01, -6.8791e-02, -1.5146e-01,\n",
      "         1.2101e-02, -4.5943e-01,  1.4300e-02, -8.7461e-02, -3.2711e-02,\n",
      "         2.4036e-01,  1.7293e-02,  1.1340e-01, -3.4248e-02,  5.0351e-02,\n",
      "         9.3530e-02, -6.4975e-02, -8.5025e-01, -1.3809e-01, -3.4919e-01,\n",
      "        -2.0540e-02, -3.7268e-01,  4.7773e-02,  4.7216e-02,  2.3236e-01,\n",
      "         1.3777e-01,  2.4962e-01,  1.3133e-01,  4.7732e-02,  4.4829e-02,\n",
      "        -1.3243e-01, -1.6702e-01,  2.1045e-01, -4.0940e-02,  3.1555e-01,\n",
      "        -5.1593e-01,  1.0297e-01,  2.9704e-01,  1.6769e-02, -2.1701e-02,\n",
      "         5.7481e-03,  6.0955e-02, -2.2314e-02,  1.6080e-01, -2.1614e-01,\n",
      "         1.0959e+00, -4.0587e-01, -1.4514e-01, -1.3610e-01,  1.1280e-01,\n",
      "         1.7697e-01, -6.5900e-02, -1.3467e-01, -5.1049e-02, -2.8790e-01,\n",
      "        -3.9537e-01,  7.9347e-02,  5.7817e-01, -1.2027e-02, -1.2462e-01,\n",
      "         4.0711e-02,  1.3596e-02,  2.0398e-01,  9.5604e-02,  6.8153e-03,\n",
      "         2.5994e-01, -1.0152e-01, -3.8128e-01,  4.2629e-01,  1.8734e-01,\n",
      "         7.3060e-03,  6.0062e-01,  2.1663e-01,  2.3836e-02,  1.2912e-02,\n",
      "        -3.0333e-01,  3.1381e-01, -2.6096e-02, -3.8907e-01,  5.5081e-02,\n",
      "        -5.0901e-02,  2.5939e-01, -2.6417e-01,  2.0716e-01,  2.2498e-01,\n",
      "         1.9117e-01,  1.2614e-01,  1.4713e-01,  1.0489e-01, -1.1291e+00,\n",
      "        -8.1722e-02,  1.2693e-01,  1.5365e-01, -8.2781e-02, -3.5168e-01,\n",
      "         1.7873e-01, -9.7911e-02, -2.5831e-01,  9.0010e-03,  3.9271e-01,\n",
      "         9.9305e-02,  2.0227e-02,  9.2149e-03,  3.3352e-01, -7.1636e-02,\n",
      "        -5.9093e-02,  9.9506e-02,  3.1135e-01,  3.1324e-01, -1.0208e-01,\n",
      "        -6.0717e-02, -4.3183e-02, -8.3102e-02,  5.3218e-01, -1.6997e-01,\n",
      "         1.1647e-01, -1.0793e-01, -3.3692e-02,  1.6272e-01,  2.0517e-01,\n",
      "         1.1426e-01, -2.0803e+00, -4.4386e-03,  8.7898e-01,  4.7096e-01,\n",
      "        -2.7657e-01, -1.9387e-01, -9.8869e-02, -1.1244e-01, -1.4206e-01,\n",
      "         9.0613e-02, -1.8396e-01,  3.6844e-02, -1.9090e-01,  8.6006e-02,\n",
      "         9.2397e-04, -4.1547e-01, -7.7672e-02,  5.0569e-01,  2.4725e-01,\n",
      "         2.4119e-01, -1.3455e-01, -3.4007e-01, -7.7146e-02, -8.4089e-02])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[10]) # this의 임베딩 벡터값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors[9999]) # seeing의 임베딩 벡터값\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 벡터들이 저장되어져 있는 TEXT.vocab.vectors를 nn.Embedding()의 초기화 입력으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0437e-01,  1.6431e-01,  4.1794e-02, -1.3708e-01, -2.9779e-01,\n",
       "          3.3440e-01, -6.9955e-02, -6.8036e-02,  1.0604e-01, -2.0337e+00,\n",
       "          1.7977e-01, -7.7403e-02, -1.9518e-01,  1.8324e-01,  3.0017e-02,\n",
       "         -5.4762e-02, -4.5725e-01, -2.4509e-02,  5.7387e-02, -3.4878e-01,\n",
       "          3.9696e-02,  4.4826e-01, -5.8462e-02,  4.1181e-01, -3.5411e-02,\n",
       "         -1.4722e-01,  1.0740e-01, -2.5896e-01, -1.1658e-01,  1.9822e-01,\n",
       "          3.2850e-01,  2.4177e-01, -5.7177e-01, -5.6442e-02, -9.6437e-01,\n",
       "          3.4482e-01,  5.4639e-02,  2.3828e-01, -1.9139e-01,  3.0899e-01,\n",
       "          2.8044e-01, -3.3814e-02, -2.5436e-01,  1.5373e-02,  1.6341e-01,\n",
       "          2.6352e-01,  1.5812e-01,  3.2044e-01, -2.3082e-01,  2.6050e-01,\n",
       "          2.0606e-01, -8.9769e-02, -1.0055e-01,  7.0378e-02, -2.7452e-02,\n",
       "          2.7959e-01, -9.5862e-02,  2.0574e-01,  2.9522e-01, -1.2285e-02,\n",
       "          1.1164e-01, -5.1373e-02,  4.6106e-01,  2.3014e-02, -3.7141e-01,\n",
       "         -2.4166e-01,  3.3773e-02,  3.6827e-02,  1.6918e-01, -1.0802e-01,\n",
       "         -1.0691e-01,  1.0219e-01,  1.0065e-01,  5.5907e-02, -2.7402e-01,\n",
       "         -1.3689e-01,  4.2095e-01, -2.4060e-02, -3.2099e-01,  3.2065e-01,\n",
       "         -1.6776e-01,  1.0170e-01,  7.4999e-02, -1.0486e-01,  3.7114e-01,\n",
       "          3.2972e-01, -3.3043e-01,  3.8343e-01,  2.4555e-01,  2.0386e-01,\n",
       "         -4.1919e-01,  1.1570e-01, -7.8632e-02, -4.3171e-01, -2.3550e-02,\n",
       "         -1.1618e-01, -2.5605e-01,  3.4693e-01,  2.0398e-01, -1.7857e-01,\n",
       "          1.7301e-01,  4.6408e-02, -1.0486e-01,  9.8706e-02, -3.2077e-02,\n",
       "         -3.0462e-01,  1.2826e-01,  6.7985e-02, -2.5993e-01,  3.8041e-01,\n",
       "          4.5252e-02, -9.1834e-02, -4.5206e-01, -1.2498e-01,  1.7515e-01,\n",
       "         -1.3551e-01, -2.0556e-03, -9.3906e-02, -2.8006e-02, -4.6975e-01,\n",
       "          8.4430e-03, -2.4092e-01,  1.6000e-01,  2.2063e-01,  3.6277e-01,\n",
       "         -6.7643e-02,  2.8755e-01,  1.2643e-01,  1.2202e-01,  1.0548e-01,\n",
       "          4.0249e-01,  2.9781e-01,  4.9507e-01, -1.1096e-01, -2.4472e-01,\n",
       "          1.8720e-01,  1.1156e-01,  1.5680e-02,  7.6296e-03,  1.4304e-01,\n",
       "         -2.9299e-01,  1.7912e-01,  1.1604e-02, -5.6776e-02, -5.0224e-01,\n",
       "         -4.7262e-01,  1.5790e-01,  3.1573e-01, -7.7839e-02,  3.5172e-01,\n",
       "          1.6097e-01, -2.2595e-01,  2.4629e-01, -3.8200e-02,  6.4918e-01,\n",
       "          5.9545e-02, -5.0641e-02,  1.9511e-01, -6.8791e-02, -1.5146e-01,\n",
       "          1.2101e-02, -4.5943e-01,  1.4300e-02, -8.7461e-02, -3.2711e-02,\n",
       "          2.4036e-01,  1.7293e-02,  1.1340e-01, -3.4248e-02,  5.0351e-02,\n",
       "          9.3530e-02, -6.4975e-02, -8.5025e-01, -1.3809e-01, -3.4919e-01,\n",
       "         -2.0540e-02, -3.7268e-01,  4.7773e-02,  4.7216e-02,  2.3236e-01,\n",
       "          1.3777e-01,  2.4962e-01,  1.3133e-01,  4.7732e-02,  4.4829e-02,\n",
       "         -1.3243e-01, -1.6702e-01,  2.1045e-01, -4.0940e-02,  3.1555e-01,\n",
       "         -5.1593e-01,  1.0297e-01,  2.9704e-01,  1.6769e-02, -2.1701e-02,\n",
       "          5.7481e-03,  6.0955e-02, -2.2314e-02,  1.6080e-01, -2.1614e-01,\n",
       "          1.0959e+00, -4.0587e-01, -1.4514e-01, -1.3610e-01,  1.1280e-01,\n",
       "          1.7697e-01, -6.5900e-02, -1.3467e-01, -5.1049e-02, -2.8790e-01,\n",
       "         -3.9537e-01,  7.9347e-02,  5.7817e-01, -1.2027e-02, -1.2462e-01,\n",
       "          4.0711e-02,  1.3596e-02,  2.0398e-01,  9.5604e-02,  6.8153e-03,\n",
       "          2.5994e-01, -1.0152e-01, -3.8128e-01,  4.2629e-01,  1.8734e-01,\n",
       "          7.3060e-03,  6.0062e-01,  2.1663e-01,  2.3836e-02,  1.2912e-02,\n",
       "         -3.0333e-01,  3.1381e-01, -2.6096e-02, -3.8907e-01,  5.5081e-02,\n",
       "         -5.0901e-02,  2.5939e-01, -2.6417e-01,  2.0716e-01,  2.2498e-01,\n",
       "          1.9117e-01,  1.2614e-01,  1.4713e-01,  1.0489e-01, -1.1291e+00,\n",
       "         -8.1722e-02,  1.2693e-01,  1.5365e-01, -8.2781e-02, -3.5168e-01,\n",
       "          1.7873e-01, -9.7911e-02, -2.5831e-01,  9.0010e-03,  3.9271e-01,\n",
       "          9.9305e-02,  2.0227e-02,  9.2149e-03,  3.3352e-01, -7.1636e-02,\n",
       "         -5.9093e-02,  9.9506e-02,  3.1135e-01,  3.1324e-01, -1.0208e-01,\n",
       "         -6.0717e-02, -4.3183e-02, -8.3102e-02,  5.3218e-01, -1.6997e-01,\n",
       "          1.1647e-01, -1.0793e-01, -3.3692e-02,  1.6272e-01,  2.0517e-01,\n",
       "          1.1426e-01, -2.0803e+00, -4.4386e-03,  8.7898e-01,  4.7096e-01,\n",
       "         -2.7657e-01, -1.9387e-01, -9.8869e-02, -1.1244e-01, -1.4206e-01,\n",
       "          9.0613e-02, -1.8396e-01,  3.6844e-02, -1.9090e-01,  8.6006e-02,\n",
       "          9.2397e-04, -4.1547e-01, -7.7672e-02,  5.0569e-01,  2.4725e-01,\n",
       "          2.4119e-01, -1.3455e-01, -3.4007e-01, -7.7146e-02, -8.4089e-02]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.LongTensor([10])) # 단어 this의 임베딩 벡터값\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
