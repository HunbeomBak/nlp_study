{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "## 분산 표현(Distributed Representation)\n",
    "- 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법 \n",
    "- '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'라는 가정\n",
    "- 원-핫 벡터처럼 벡터의 차원이 단어 집합(vocabulary)의 크기일 필요가 없으므로, 벡터의 차원이 상대적으로 저차원으로 줄어듬\n",
    "- 단어의 의미를 여러 차원에다가 분산하여 표현\n",
    "- 단어 간 유사도를 계산할 수 있음\n",
    "- NNLM, RNNLM, Word2Vec\n",
    "\n",
    "## CBOW(Continuous Bag of Words)\n",
    "- CBOW는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법\n",
    "- Ex : \"The fat cat sat on the mat\" => 주변 단어(context word) {\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat\"}으로부터 중심 단어(center word) sat을 예측\n",
    "- window : 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정하는 범위\n",
    "- sliding window : 윈도우를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터 셋을 만듬\n",
    "- 입력 : 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터\n",
    "- 출력 : 예측하고자 하는 중간 단어의 원-핫 벡터\n",
    "- 딥러닝이 아니라 1개의 은닉층만을 사용한 Shallow Neural Network + 활성화 함수 존재하지 않음\n",
    "- loss : cross entropy\n",
    "\n",
    "## Skip-gram\n",
    "- Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법\n",
    "\n",
    "## 네거티브 샘플링(Negative Sampling)\n",
    "Skip-gram으로 학습할때, 출력층에 있는 소프트맥스 함수는 단어 집합 크기의 벡터 내의 모든 값을 0과 1사이의 값이면서 모두 더하면 1이 되도록 바꾸는 작업을 수행 \n",
    "- 단어 집합의 크기가 수백만에 달한다면 이 작업은 굉장히 무거운 작업\n",
    "- 역전파를 수행하므로 주변 단어와 상관 없는 모든 단어까지의 워드 임베딩 조정 작업을 수행 => 관계가 없는 수많은 단어의 임베딩을 조정할 필요가 없음\n",
    "\n",
    "위 문제를 해결하기 위해 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 바꿈 \n",
    "- 주변단어로 이루어진 집합 A와 랜덤으로 선택된 주변단어가 아닌 집합 B\n",
    "- 주변 단어들을 긍정(positive)으로 두고 랜덤으로 샘플링 된 단어들을 부정(negative)으로 둔 다음에 이진 분류 문제를 수행\n",
    "\n",
    "## 영어 Word2Vec 만들기\n",
    "\n",
    "### 훈련 데이터 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x1ad63080130>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리\n",
    "- 필요한 데이터는 영어문장으로만 구성된 내용을 담고 있는 < content >와 < /content > 사이의 내용\n",
    "- xml 문법들은 제거\n",
    "- (Laughter)나 (Applause)와 같은 배경음을 나태나는 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import zipfile\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()')) # xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text) # 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = sent_tokenize(content_text)\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 273424\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "\n",
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]: # 샘플 3개만 출력\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "- window = 컨텍스트 윈도우 크기\n",
    "- min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "- workers = 학습을 위한 프로세스 수\n",
    "- sg = 0은 CBOW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8305555582046509), ('guy', 0.8077905178070068), ('lady', 0.7679110765457153), ('boy', 0.7539355754852295), ('gentleman', 0.7429524660110474), ('girl', 0.7246814370155334), ('soldier', 0.6980609893798828), ('kid', 0.6841074228286743), ('surgeon', 0.6815424561500549), ('poet', 0.6681637763977051)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "man과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid 등을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 모델 저장하고 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('./eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8305555582046509), ('guy', 0.8077905178070068), ('lady', 0.7679110765457153), ('boy', 0.7539355754852295), ('gentleman', 0.7429524660110474), ('girl', 0.7246814370155334), ('soldier', 0.6980609893798828), ('kid', 0.6841074228286743), ('surgeon', 0.6815424561500549), ('poet', 0.6681637763977051)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 벡터의 시각화(Embedding Visualization)\n",
    "\n",
    "### 워드 임베딩 모델로부터 2개의 tsv 파일 생성\n",
    "python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름   \n",
    "python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v을 cmd에서 실행\n",
    "\n",
    "### 임베딩 프로젝터를 사용하여 시각화하기\n",
    "구글의 [Embedding Projector](https://projector.tensorflow.org/)을 사용하여 워드 임베딩 모델 시각화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
