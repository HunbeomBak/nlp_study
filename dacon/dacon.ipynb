{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c270a5f4b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  He was almost choking. There was so much, so m...       3\n",
       "1             “Your sister asked for it, I suppose?”       2\n",
       "2   She was engaged one day as she walked, in per...       1\n",
       "3  The captain was in the porch, keeping himself ...       4\n",
       "4  “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/train.csv', usecols=['text','author'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(df['author'].unique())\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리\n",
    "### 텍스트 전처리\n",
    "- 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_num(text):\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking There was so much so muc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your sister asked for it I suppose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked in peru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch keeping himself c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have mercy gentlemen odin flung up his hands D...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  He was almost choking There was so much so muc...       3\n",
       "1                 Your sister asked for it I suppose       2\n",
       "2   She was engaged one day as she walked in peru...       1\n",
       "3  The captain was in the porch keeping himself c...       4\n",
       "4  Have mercy gentlemen odin flung up his hands D...       3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['text'] = df['text'].str.lower() # 전부다 소문자로 변경 / 나중에 torchtext에서 해주기때문에 안해도됨\n",
    "df['text'] = df['text'].apply(alpha_num) # 특수문자 제거\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 분리\n",
    "sklearn의 train_test_split으로 분리하고 각각 csv 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43903\n",
      "10976\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "#train_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "#print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "val_df.to_csv(\"val_data.csv\", index=False)\n",
    "#test_df.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HunbeomBak\\.conda\\envs\\nlp\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(sequential=True,\n",
    "                  use_vocab=True,\n",
    "                  tokenize=str.split,\n",
    "                  lower=True,\n",
    "                  batch_first=True,\n",
    "                  fix_length=20)\n",
    "\n",
    "LABEL = data.Field(sequential=False,\n",
    "                   use_vocab=False,\n",
    "                   batch_first=False,\n",
    "                   is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HunbeomBak\\.conda\\envs\\nlp\\lib\\site-packages\\torchtext\\data\\example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "C:\\Users\\HunbeomBak\\.conda\\envs\\nlp\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data= TabularDataset.splits(\n",
    "    path = '.', \n",
    "    train='train_data.csv', \n",
    "    validation='val_data.csv', \n",
    "    #test='test_data.csv', \n",
    "    format='csv',\n",
    "    fields=[('text', TEXT), ('label', LABEL)],\n",
    "    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : 43903\n",
      "검증 데이터의 크기 : 10976\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 크기 : {}' .format(len(train_data)))\n",
    "print('검증 데이터의 크기 : {}' .format(len(val_data)))\n",
    "#print('테스트 데이터의 크기 : {}' .format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['he', 'was', 'almost', 'choking', 'there', 'was', 'so', 'much', 'so', 'much', 'he', 'wanted', 'to', 'say', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips', 'the', 'pole', 'gazed', 'fixedly', 'at', 'him', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', 'looked', 'at', 'odin', 'and', 'was', 'in', 'evident', 'perplexity'], 'label': '3'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['why', 'you', 'seem', 'to', 'take', 'me', 'for', 'little', 'odin', 'said', 'odin', 'with', 'a', 'grin', 'of', 'irritation', 'but', 'please', 'dont', 'suppose', 'i', 'am', 'such', 'a', 'revolutionist', 'i', 'often', 'disagree', 'with', 'mr', 'odin', 'though', 'i', 'mention', 'tatyana', 'i', 'am', 'not', 'at', 'all', 'for', 'the', 'emancipation', 'of', 'women', 'i', 'acknowledge', 'that', 'women', 'are', 'a', 'subject', 'race', 'and', 'must', 'obey', 'les', 'femmes', 'tricottent', 'as', 'napoleon', 'said', 'odin', 'for', 'some', 'reason', 'smiled', 'and', 'on', 'that', 'question', 'at', 'least', 'i', 'am', 'quite', 'of', 'one', 'mind', 'with', 'that', 'pseudogreat', 'man', 'i', 'think', 'too', 'that', 'to', 'leave', 'ones', 'own', 'country', 'and', 'fly', 'to', 'america', 'is', 'mean', 'worse', 'than', 'meansilly', 'why', 'go', 'to', 'america', 'when', 'one', 'may', 'be', 'of', 'great', 'service', 'to', 'humanity', 'here', 'now', 'especially', 'theres', 'a', 'perfect', 'mass', 'of', 'fruitful', 'activity', 'open', 'to', 'us', 'thats', 'what', 'i', 'answered'], 'label': '3'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(val_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary 생성\n",
    "## 직접 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 10002\n",
      "클래스의 개수 : 5\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=5, max_size=10000)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('클래스의 개수 : {}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 686\n",
      "검증 데이터의 미니 배치의 개수 : 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HunbeomBak\\.conda\\envs\\nlp\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "    (train_data, val_data), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    repeat=False,\n",
    "    sort=False)\n",
    "\n",
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "#print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### device 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "#DEVICE = 'cpu'\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화\n",
    "        x, _ = self.gru(x, h_0)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
    "        h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(3, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        #y.data.sub_(1)  # 레이블 값을 0과 1로 변환\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    \"\"\"evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for b, batch in enumerate(val_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        #y.data.sub_(1) # 레이블 값을 0과 1로 변환\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    #print(size)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HunbeomBak\\.conda\\envs\\nlp\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] val loss :  1.33 | val accuracy : 44.73\n",
      "[Epoch: 2] val loss :  1.22 | val accuracy : 51.26\n",
      "[Epoch: 3] val loss :  1.16 | val accuracy : 53.02\n",
      "[Epoch: 4] val loss :  1.11 | val accuracy : 55.70\n",
      "[Epoch: 5] val loss :  1.08 | val accuracy : 57.63\n",
      "[Epoch: 6] val loss :  1.06 | val accuracy : 58.78\n",
      "[Epoch: 7] val loss :  1.06 | val accuracy : 58.34\n",
      "[Epoch: 8] val loss :  1.03 | val accuracy : 60.42\n",
      "[Epoch: 9] val loss :  1.04 | val accuracy : 60.33\n",
      "[Epoch: 10] val loss :  1.01 | val accuracy : 61.97\n",
      "[Epoch: 11] val loss :  1.03 | val accuracy : 61.56\n",
      "[Epoch: 12] val loss :  1.03 | val accuracy : 62.29\n",
      "[Epoch: 13] val loss :  1.06 | val accuracy : 62.54\n",
      "[Epoch: 14] val loss :  1.08 | val accuracy : 62.31\n",
      "[Epoch: 15] val loss :  1.10 | val accuracy : 62.57\n",
      "[Epoch: 16] val loss :  1.13 | val accuracy : 61.95\n",
      "[Epoch: 17] val loss :  1.14 | val accuracy : 63.03\n",
      "[Epoch: 18] val loss :  1.18 | val accuracy : 62.25\n",
      "[Epoch: 19] val loss :  1.21 | val accuracy : 61.86\n",
      "[Epoch: 20] val loss :  1.25 | val accuracy : 63.03\n",
      "[Epoch: 21] val loss :  1.23 | val accuracy : 63.31\n",
      "[Epoch: 22] val loss :  1.32 | val accuracy : 63.33\n",
      "[Epoch: 23] val loss :  1.38 | val accuracy : 62.43\n",
      "[Epoch: 24] val loss :  1.41 | val accuracy : 62.67\n",
      "[Epoch: 25] val loss :  1.44 | val accuracy : 62.54\n",
      "[Epoch: 26] val loss :  1.49 | val accuracy : 62.79\n",
      "[Epoch: 27] val loss :  1.57 | val accuracy : 62.30\n",
      "[Epoch: 28] val loss :  1.62 | val accuracy : 62.58\n",
      "[Epoch: 29] val loss :  1.66 | val accuracy : 62.68\n",
      "[Epoch: 30] val loss :  1.70 | val accuracy : 62.99\n",
      "[Epoch: 31] val loss :  1.75 | val accuracy : 62.99\n",
      "[Epoch: 32] val loss :  1.80 | val accuracy : 62.69\n",
      "[Epoch: 33] val loss :  1.83 | val accuracy : 62.47\n",
      "[Epoch: 34] val loss :  1.88 | val accuracy : 62.45\n",
      "[Epoch: 35] val loss :  1.99 | val accuracy : 62.54\n",
      "[Epoch: 36] val loss :  2.01 | val accuracy : 61.94\n",
      "[Epoch: 37] val loss :  2.00 | val accuracy : 62.01\n",
      "[Epoch: 38] val loss :  2.08 | val accuracy : 62.48\n",
      "[Epoch: 39] val loss :  2.15 | val accuracy : 61.88\n",
      "[Epoch: 40] val loss :  2.12 | val accuracy : 62.58\n",
      "[Epoch: 41] val loss :  2.24 | val accuracy : 62.08\n",
      "[Epoch: 42] val loss :  2.22 | val accuracy : 62.35\n",
      "[Epoch: 43] val loss :  2.25 | val accuracy : 62.14\n",
      "[Epoch: 44] val loss :  2.39 | val accuracy : 62.32\n",
      "[Epoch: 45] val loss :  2.32 | val accuracy : 62.61\n",
      "[Epoch: 46] val loss :  2.40 | val accuracy : 62.30\n",
      "[Epoch: 47] val loss :  2.36 | val accuracy : 61.88\n",
      "[Epoch: 48] val loss :  2.49 | val accuracy : 62.15\n",
      "[Epoch: 49] val loss :  2.43 | val accuracy : 62.35\n",
      "[Epoch: 50] val loss :  2.46 | val accuracy : 62.43\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "\n",
    "    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\n",
    "\n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), './snapshot/dacon.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
